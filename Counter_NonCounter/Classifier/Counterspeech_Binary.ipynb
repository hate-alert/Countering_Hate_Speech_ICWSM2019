{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task -1: Classifying Text into Counter and Non Counter Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dbfef248-307f-47da-8631-cda397947e42"
    }
   },
   "source": [
    "### This notebook is used to measure performance of the combination of different classifier and different feature engineering techniques  used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem.porter import *\n",
    "ps = PorterStemmer()\n",
    "from scipy.sparse import vstack, hstack\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "####features module has the necessary function for feature generation \n",
    "from utils.features import *\n",
    "###tokenize module has the tokenization funciton\n",
    "from utils.tokenize import *\n",
    "###helper prints confusion  matrix and stores results\n",
    "from utils.helper import *\n",
    "###common preprocessing imports\n",
    "from utils.commen_preprocess import *\n",
    "\n",
    "####gensim load \n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "9e31b494-059b-489a-93fc-4183be2b1816"
    }
   },
   "outputs": [],
   "source": [
    "###ipywigets\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word_to_vec model  loading \n",
    "1. change the path of glove model file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "GLOVE_MODEL_FILE=\"../../../embeddings/glove.840B.300d.txt\"\n",
    "print(os.path.isfile(GLOVE_MODEL_FILE))\n",
    "\n",
    "## change the embedding dimension according to the model\n",
    "EMBEDDING_DIM = 300\n",
    "def loadGloveModel2(glove_file):\n",
    "    tmp_file = get_tmpfile(\"test_crawl_200.txt\")\n",
    "\n",
    "    # call glove2word2vec script\n",
    "    # default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    model=KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    return model\n",
    "\n",
    "word2vec_model = loadGloveModel2(GLOVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2028ea9b-4304-497f-8aaf-d9bab5768473"
    }
   },
   "source": [
    "## Dataset is loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "6ffa7914-4cfc-4896-bf29-cd911f4f940e"
    }
   },
   "outputs": [],
   "source": [
    "### change the path where the data is kept\n",
    "path='../../Data/Counterspeech_Dataset.json'\n",
    "with open(path) as fp:\n",
    "    train_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "cc5af626-9275-4864-a427-c3313f75ebec"
    }
   },
   "outputs": [],
   "source": [
    "def convert_class_label(input_text):\n",
    "    if input_text:\n",
    "        return 'counter'\n",
    "    else:\n",
    "        return 'noncounter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data in the dataframe  having the four fields as\n",
    "1. id\n",
    "2. class\n",
    "3. community\n",
    "4. category(labels)\n",
    "5. text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "7116d4be-df51-41f3-ac7e-c48995ca0e31"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loading Completed...\n"
     ]
    }
   ],
   "source": [
    "pd_train = pd.DataFrame(columns=['id','class','community','category','text'])\n",
    "\n",
    "for count, each in enumerate(train_data):\n",
    "    try:\n",
    "        pd_train.loc[count]  = [each['id'], convert_class_label(each['CounterSpeech']), each['Community'],each['Category'],each['commentText']]\n",
    "    except:\n",
    "        pass\n",
    "print('Training Data Loading Completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "c8d189bd-9c72-48fb-afee-cec1ff7712c4"
    }
   },
   "outputs": [],
   "source": [
    "pd_train['text'].replace('', np.nan, inplace=True)\n",
    "pd_train.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "b5db1bd0-6dc5-4393-9ef4-3f7ea96278ea"
    }
   },
   "outputs": [],
   "source": [
    "#### converting the data into text and labels dictionary\n",
    "def get_data(pd_train):\n",
    "    comments=pd_train['text'].values\n",
    "    labels=pd_train['class'].values\n",
    "    list_comment=[]\n",
    "    for comment,label in zip(comments,labels):\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        temp['label']=label\n",
    "        list_comment.append(temp)\n",
    "    return list_comment    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(m_type=None):\n",
    "    if not m_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if m_type == 'decision_tree_classifier':\n",
    "        logreg = tree.DecisionTreeClassifier(class_weight='balanced')\n",
    "    elif m_type == 'MLPClassifier':\n",
    "        logreg = neural_network.MLPClassifier((500))\n",
    "    elif m_type == 'KNeighborsClassifier':\n",
    "        logreg = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "    elif m_type == 'ExtraTreeClassifier':\n",
    "        logreg = tree.ExtraTreeClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier_2':\n",
    "        logreg = ensemble.ExtraTreesClassifier()\n",
    "    elif m_type == 'RandomForestClassifier':\n",
    "        logreg = ensemble.RandomForestClassifier(class_weight='balanced')\n",
    "    elif m_type == 'Logistic_Regression':\n",
    "        logreg = linear_model.LogisticRegression(class_weight='balanced')\n",
    "    elif m_type == 'SVC':\n",
    "        logreg = SVC(class_weight='balanced');\n",
    "    elif m_type == 'Catboost':\n",
    "        logreg = CatBoostClassifier(iterations=100,scale_pos_weight=(4048/5335))\n",
    "    elif m_type == 'XGB_classifier':\n",
    "        logreg=XGBClassifier(scale_pos_weight=(4048/5335),n_estimators=500,nthread=12)\n",
    "    elif m_type == 'Gaussian_NB':\n",
    "        logreg = GaussianNB()\n",
    "    else:\n",
    "        print(\"give correct model\")\n",
    "        \n",
    "    return logreg\n",
    "\n",
    "def get_feature(pd_train,f_type=None):\n",
    "    if not f_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None,None\n",
    "    if f_type == 'google_not_preprocess':\n",
    "        X,y=gen_data_google2(pd_train)\n",
    "    elif f_type == 'word_to_vec_embed':\n",
    "        X,y=gen_data_embed(pd_train,word2vec_model)\n",
    "    elif f_type == 'google_preprocess':\n",
    "        X,y=gen_data_google(pd_train)\n",
    "    elif f_type == 'tfidf_not_preprocess':\n",
    "        X,y=gen_data_new_tfidf2(pd_train)\n",
    "    elif f_type == 'tfidf_preprocess':\n",
    "        X,y=gen_data_new_tfidf(pd_train)\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_google_rem(pd_train)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_google_norem(pd_train)\n",
    "    elif f_type == 'google_preprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_norem_google_rem(pd_train)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_rem_google_norimportem(pd_train)\n",
    "    elif f_type == 'google_preprocess_embed':\n",
    "        X,y=combine_google_rem_embed(pd_train,word2vec_model)\n",
    "    elif f_type == 'tfidf_preprocess_embed':\n",
    "        X,y=combine_tf_rem_embed(pd_train,word2vec_model)\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess_embed':\n",
    "        ###best features####\n",
    "        X,y=combine_tf_rem_google_rem_embed(pd_train,word2vec_model)\n",
    "    else:\n",
    "        print(\"give correct feature selection\")    \n",
    "    return X,y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T12:55:02.194968Z",
     "start_time": "2019-01-17T12:55:02.187501Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('binary_all_parameters.json') as f:\n",
    "        parameters=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "fb2362a2-e530-4883-8d3a-44266b955ec1"
    }
   },
   "outputs": [],
   "source": [
    "def classification_model(pd_train,classifier_model,feature_model,img_name,report_name,save_model=False):\n",
    "    X,y=get_feature(pd_train,f_type=feature_model)\n",
    "    model=get_model(m_type=classifier_model)\n",
    "    if(model==None):\n",
    "        return 1\n",
    "    try:\n",
    "        model_parameter=parameters[classifier_model+'+'+feature_model]\n",
    "        for k,v in param_set.items():\n",
    "            setattr(model_parameter,k,v)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    Classifier_Train_X = np.array(X, copy=False)\n",
    "    Classifier_Train_Y = y\n",
    "    label_map = {\n",
    "            'counter': 0,\n",
    "            'noncounter': 1\n",
    "        }\n",
    "    temp=[]\n",
    "    for data in Classifier_Train_Y:\n",
    "            temp.append(label_map[data])\n",
    "\n",
    "    Classifier_Train_Y=np.array(temp)\n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=model\n",
    "        Classifier.fit(Classifier_Train_X, Classifier_Train_Y)\n",
    "        filename = '../../Best_model/'+classifier_model+'_task_1.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "        \n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=10)\n",
    "        y_total_preds=[] \n",
    "        y_total=[]\n",
    "        count=0\n",
    "\n",
    "        for train_index, test_index in kf.split(Classifier_Train_X,Classifier_Train_Y):\n",
    "            print('cv_fold',count)\n",
    "            X_train, X_test = Classifier_Train_X[train_index], Classifier_Train_X[test_index]\n",
    "            y_train, y_test = Classifier_Train_Y[train_index], Classifier_Train_Y[test_index]\n",
    "            classifier=model \n",
    "            classifier.fit(X_train,y_train)\n",
    "            y_preds = classifier.predict(X_test)\n",
    "            for ele in y_test:\n",
    "                y_total.append(ele)\n",
    "            for ele in y_preds:\n",
    "                y_total_preds.append(ele)\n",
    "            y_pred_train = classifier.predict(X_train)\n",
    "            count=count+1       \n",
    "            print('accuracy_train:',accuracy_score(y_train, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds))\n",
    "\n",
    "        report = classification_report( y_total, y_total_preds )\n",
    "        cm=confusion_matrix(y_total, y_total_preds)\n",
    "        plt=plot_confusion_matrix(cm,normalize= True,target_names = ['counter','non_counter'],title = \"Confusion Matrix\")\n",
    "        plt.savefig(img_name)\n",
    "        print(report)\n",
    "        df_result=pandas_classification_report(y_total,y_total_preds)\n",
    "        df_result.to_csv(report_name,  sep=',')\n",
    "        with open('all_preds_binary.pkl', 'wb') as f:\n",
    "              pickle.dump([y_total,y_total_preds], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models available\n",
    "1. decision_tree_classifier\n",
    "2. MLPClassifier\n",
    "3. KNeighborsClassifier\n",
    "4. ExtraTreeClassifier\n",
    "5. ExtraTreeClassifier_2\n",
    "6. RandomForestClassifier\n",
    "7. SVC\n",
    "8. Catboost\n",
    "9. XGB_classifier\n",
    "10. Logistic Regression\n",
    "11. Gaussian Naive bayes\n",
    "\n",
    "## Feature Models available\n",
    "1. google_not_preprocess\n",
    "2. word_to_vec_embed\n",
    "3. google_preprocess\n",
    "4. tfidf_not_preprocess\n",
    "5. tfidf_preprocess\n",
    "6. google_preprocess_tfidf_preprocess\n",
    "7. google_nopreprocess_tfidf_nopreprocess\n",
    "8. google_preprocess_tfidf_nopreprocess\n",
    "9. google_nopreprocess_tfidf_preprocess\n",
    "10. google_preprocess_embed\n",
    "11. tfidf_preprocess_embed\n",
    "12. google_preprocess_tfidf_preprocess_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the model and the feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select feature combination\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1a7cd3627f492e8d5cb2bf4a33ab20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(options=('google_not_preprocess', 'word_to_vec_embed', 'google_preprocess', 'tfidf_not_preprocess', 'tfidf_preprocess', 'google_preprocess_tfidf_preprocess', 'google_nopreprocess_tfidf_nopreprocess', 'google_preprocess_tfidf_nopreprocess', 'google_nopreprocess_tfidf_preprocess', 'google_preprocess_embed', 'tfidf_preprocess_embed', 'google_preprocess_tfidf_preprocess_embed'), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options_ft=['google_not_preprocess','word_to_vec_embed','google_preprocess','tfidf_not_preprocess','tfidf_preprocess','google_preprocess_tfidf_preprocess','google_nopreprocess_tfidf_nopreprocess','google_preprocess_tfidf_nopreprocess', 'google_nopreprocess_tfidf_preprocess','google_preprocess_embed','tfidf_preprocess_embed','google_preprocess_tfidf_preprocess_embed']\n",
    "ft= widgets.Dropdown(options=options_ft, value=None)\n",
    "print('select feature combination') \n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select a model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1e529fc80143d797990ac67e3f023b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(options=('decision_tree_classifier', 'MLPClassifier', 'KNeighborsClassifier', 'ExtraTreeClassifier', 'ExtraTreeClassifier_2', 'RandomForestClassifier', 'SVC', 'Catboost', 'XGB_classifier', 'Logistic_Regression', 'Gaussian Naive bayes'), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options_clf=['decision_tree_classifier','MLPClassifier','KNeighborsClassifier','ExtraTreeClassifier','ExtraTreeClassifier_2','RandomForestClassifier','SVC','Catboost','XGB_classifier','Logistic_Regression','Gaussian Naive bayes']\n",
    "clf= widgets.Dropdown(options=options_clf, value=None)\n",
    "print('select a model') \n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###specify the model name\n",
    "clf_model=clf.value\n",
    "###specify the feature model###\n",
    "ft_model=ft.value\n",
    "###image_name###\n",
    "im=clf_model+ft_model+'_cm.png'\n",
    "###report_name###\n",
    "re=clf_model+ft_model+'_report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "966be667-e453-41bd-a97e-73649afc28c4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fifti'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###actual classfier call\n",
    "classification_model(pd_train,classifier_model=clf_model,feature_model=ft_model,img_name=im,report_name=re,save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:punyajoy-nogpu]",
   "language": "python",
   "name": "conda-env-punyajoy-nogpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
