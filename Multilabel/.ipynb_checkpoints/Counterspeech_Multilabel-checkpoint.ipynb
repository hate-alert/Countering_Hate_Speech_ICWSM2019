{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task- 3: Multilabel Classification of Counter Speech\n",
    "### This notebook is used to measure performance of the combination of different classifier and different feature engineering techniques  used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commen_preprocess import *\n",
    "from skmultilearn.adapt import brknn\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.problem_transform import br\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.neurofuzzy import MLARAM\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "import argparse\n",
    "import numpy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import gensim, sklearn\n",
    "from collections import defaultdict\n",
    "#from batch_gen import batch_gen\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import os\n",
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import confusion_matrix,make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "ps = PorterStemmer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.sparse import vstack, hstack\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "####features module has the necessary function for feature generation \n",
    "from utils.multi_features import *\n",
    "###tokenize module has the tokenization funciton\n",
    "from utils.tokenize import *\n",
    "###helper prints confusion  matrix and stores results\n",
    "from utils.helper import *\n",
    "###common preprocessing imports\n",
    "from utils.commen_preprocess import *\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word_to_vec model  loading \n",
    "1. change the path of glove model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "GLOVE_MODEL_FILE=\"../../embeddings/glove.840B.300d.txt\"\n",
    "print(os.path.isfile(GLOVE_MODEL_FILE))\n",
    "\n",
    "## change the embedding dimension according to the model\n",
    "EMBEDDING_DIM = 300\n",
    "def loadGloveModel2(glove_file):\n",
    "    tmp_file = get_tmpfile(\"test_crawl_200.txt\")\n",
    "\n",
    "    # call glove2word2vec script\n",
    "    # default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    model=KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    return model\n",
    "\n",
    "word2vec_model = loadGloveModel2(GLOVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset is loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the path of the the json file where to load the data \n",
    "path='../Data/Counterspeech_Dataset.json'\n",
    "with open(path) as fp:\n",
    "    train_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### return label\n",
    "def convert_class_label(input_text):\n",
    "    if input_text:\n",
    "        return 'counter'\n",
    "    else:\n",
    "        return 'noncounter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data in the dataframe  having the four fields as\n",
    "1. id\n",
    "2. class\n",
    "3. community\n",
    "4. category(labels)\n",
    "5. text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loading Completed...\n"
     ]
    }
   ],
   "source": [
    "pd_train = pd.DataFrame(columns=['id','class','community','category','text'])\n",
    "\n",
    "for count, each in enumerate(train_data):\n",
    "    try:\n",
    "        pd_train.loc[count]  = [each['id'], convert_class_label(each['CounterSpeech']), each['Community'],each['Category'],each['commentText']]\n",
    "    except:\n",
    "        pass\n",
    "print('Training Data Loading Completed...') \n",
    "\n",
    "### select the data having the labels ...default denotes the non counter speech\n",
    "pd_train =pd_train[pd_train['category']!='Default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of the category column into a multi-hot vector encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[[],[],[],[],[],[],[],[],[],[]]\n",
    "for ele in pd_train['category']:\n",
    "    temp=[]\n",
    "    if type(ele) is int:\n",
    "        ele =str(ele)\n",
    "    for i in range(0,len(ele),2):\n",
    "        temp.append(ord(ele[i])-ord('0'))\n",
    "    #print(temp)\n",
    "    if(len(temp)==0):\n",
    "        print(temp)\n",
    "    for i in range(0,10):\n",
    "        if i+1 in temp:\n",
    "            list1[i].append(1)\n",
    "        else:\n",
    "            list1[i].append(0)\n",
    "y_train=np.array([np.array(xi) for xi in list1])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### final dataframe for the task created \n",
    "pd_train = pd.DataFrame({'text':list(pd_train['text']),'cat0':list1[0],'cat1':list1[1],'cat2':list1[2],'cat3':list1[3],'cat4':list1[4],'cat5':list1[5],'cat6':list1[6],'cat7':list1[7],'cat8':list1[8],'cat9':list1[9]})\n",
    "### drop the entries having blank entries\n",
    "pd_train['text'].replace('', np.nan, inplace=True)\n",
    "pd_train.dropna(subset=['text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What kind of God is it that Hates people &amp; Cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Question - Which comment is racist and why?\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>That Israel fail that is nothing new. That Isr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This god damned ignorant little prick doesn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>our shit may be cooler but... police brutality...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  \\\n",
       "0     0     1     0     0     0     0     0     1     0     0   \n",
       "1     0     0     0     0     0     0     0     1     0     0   \n",
       "2     0     0     0     0     0     0     0     1     0     0   \n",
       "3     0     0     0     0     0     0     0     0     1     0   \n",
       "4     0     1     0     0     0     0     0     0     0     0   \n",
       "\n",
       "                                                text  \n",
       "0  What kind of God is it that Hates people & Cur...  \n",
       "1  Question - Which comment is racist and why?\\n ...  \n",
       "2  That Israel fail that is nothing new. That Isr...  \n",
       "3  This god damned ignorant little prick doesn't ...  \n",
       "4  our shit may be cooler but... police brutality...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### converting the data into text and labels dictionary\n",
    "def get_data():\n",
    "    comments=pd_train['text'].values\n",
    "    df = pd_train.drop('text', 1)\n",
    "    labels=df.values\n",
    "    list_comment=[]\n",
    "    for comment,label in zip(comments,labels):\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        temp['label']=label\n",
    "        list_comment.append(temp)\n",
    "    return list_comment    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different feature generation methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('multilabel_all_parameters.json') as f:\n",
    "        parameters=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(model_name,featureset_name,model_type=None,save_model=False):\n",
    "    X,Y=get_feature(pd_train,featureset_name)\n",
    "    model=get_model(model_type)\n",
    "    try:\n",
    "        model_parameter=parameters[classifier_model+'+'+feature_model]\n",
    "        for k,v in param_set.items():\n",
    "             setattr(model_parameter,k,v)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if(model==None):\n",
    "        return 1\n",
    "    path=model_name+'_'+featureset_name\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
    "    X = numpy.array(X)\n",
    "    Y = numpy.array(Y)\n",
    "    \n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=model\n",
    "        Classifier.fit(X, Y)\n",
    "        filename = '../../Best_model/'+classifier_model+'_task_3.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "    \n",
    "    else:\n",
    "        ham_list=[]\n",
    "        acc_list=[]\n",
    "        pre_list=[]\n",
    "        rec_list=[]\n",
    "        f1_list=[]\n",
    "        hard_metric_list=[]\n",
    "        hard_train_list=[]\n",
    "        y_total_preds=[] \n",
    "        y_total=[]\n",
    "        count=1\n",
    "        for train_index, test_index in mskf.split(X, Y):\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                print(\"crossval step--\",str(count))\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = Y[train_index], Y[test_index]    \n",
    "                clf = model\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred =  clf.predict(X_test)\n",
    "                y_train_pred =  clf.predict(X_train)\n",
    "\n",
    "                if(scipy.sparse.issparse(y_pred)):\n",
    "                   ham,acc,pre,rec,f1=calculate_score(y_test,y_pred.toarray())\n",
    "                   for ele in y_test:\n",
    "                      y_total.append(ele)\n",
    "                   for ele in y_pred.toarray():\n",
    "                      y_total_preds.append(ele)\n",
    "                   accuracy_train=accuracy_score(y_train,y_train_pred.toarray())\n",
    "                   accuracy_test=accuracy_score(y_test,y_pred.toarray())\n",
    "\n",
    "                else:\n",
    "                   ham,acc,pre,rec,f1=calculate_score(y_test,y_pred)\n",
    "                   for ele in y_test:\n",
    "                      y_total.append(ele)\n",
    "                   for ele in y_pred:\n",
    "                      y_total_preds.append(ele)\n",
    "                   accuracy_train=my_accuracy_score(y_train,y_train_pred)\n",
    "                   accuracy_test=my_accuracy_score(y_test,y_pred)\n",
    "\n",
    "                ham_list.append(ham)\n",
    "                acc_list.append(acc)\n",
    "                pre_list.append(pre)\n",
    "                rec_list.append(rec)\n",
    "                f1_list.append(f1)\n",
    "                hard_train_list.append(accuracy_train)\n",
    "                hard_metric_list.append(accuracy_test)\n",
    "                count=count+1\n",
    "        y_total_preds=np.array(y_total_preds) \n",
    "        y_total=np.array(y_total)\n",
    "        with open('all_preds_multilabel.pkl', 'wb') as f:\n",
    "            pickle.dump([y_total,y_total_preds], f)\n",
    "\n",
    "        for i in range(10):\n",
    "            df_result=pandas_classification_report(y_total[:,i],y_total_preds[:,i])\n",
    "            df_result.to_csv(path+'/report'+str(i)+'.csv')\n",
    "\n",
    "\n",
    "        f = open(path+'/final_report.txt', \"w\")\n",
    "        f.write(model_name)    \n",
    "        f.write(\"The hard train score is :- \" + str(numpy.mean(hard_train_list)))\n",
    "        f.write(\"The hard metric score is :- \" + str(numpy.mean(hard_metric_list)))\n",
    "        f.write(\"The accuracy is :- \" + str(numpy.mean(acc_list)))\n",
    "        f.write(\"The precision is :- \" + str(numpy.mean(pre_list)))\n",
    "        f.write(\"The recall is :- \" + str(numpy.mean(rec_list)))\n",
    "        f.write(\"The f1_score is :- \" + str(numpy.mean(f1_list)))\n",
    "        f.write(\"The hamming loss is :-\" + str(numpy.mean(ham_list)))\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(m_type=None):\n",
    "    if not m_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if m_type == 'decision_tree_classifier':\n",
    "        logreg = tree.DecisionTreeClassifier(class_weight='balanced')\n",
    "    elif m_type == 'MLPClassifier':\n",
    "        logreg = neural_network.MLPClassifier((500))\n",
    "    elif m_type == 'KNeighborsClassifier':\n",
    "        logreg = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "    elif m_type == 'ExtraTreeClassifier':\n",
    "        logreg = tree.ExtraTreeClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier_2':\n",
    "        logreg = ensemble.ExtraTreesClassifier()\n",
    "    elif m_type == 'Logistic_Regression':\n",
    "        logreg = OneVsRestClassifier(linear_model.LogisticRegression(class_weight='balanced'))\n",
    "    elif m_type == 'RandomForestClassifier':\n",
    "        logreg = ensemble.RandomForestClassifier(class_weight='balanced')\n",
    "    elif m_type == 'binary_relevance_GaussianNB':\n",
    "        logreg = OneVsRestClassifier(GaussianNB())\n",
    "    elif m_type == 'SVC':\n",
    "        logreg = OneVsRestClassifier(SVC(class_weight='balanced'))\n",
    "    elif m_type == 'XG_BOOST':\n",
    "        ###best_model####\n",
    "        logreg = OneVsRestClassifier(XGBClassifier(eval_metric='logloss',objective= 'binary:logistic', nthread=12, scale_pos_weight=5))\n",
    "    elif m_type == 'Catboost':\n",
    "        logreg = OneVsRestClassifier(CatBoostClassifier(iterations=100,silent=True,scale_pos_weight=5))\n",
    "    elif m_type == 'MLKNN':\n",
    "        logreg = MLkNN(k=8)\n",
    "    elif m_type == 'MLARAM':\n",
    "        logreg = MLARAM(threshold=0.05, vigilance=0.95)\n",
    "    else:\n",
    "        print(\"ERROR: Please specify a correct model\")\n",
    "        return None\n",
    "\n",
    "    return logreg\n",
    "\n",
    "\n",
    "def get_feature(pd_train,f_type=None):\n",
    "    if not f_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None,None\n",
    "    if f_type == 'google_not_preprocess':\n",
    "        X,y=gen_data_google2(pd_train)\n",
    "    elif f_type == 'word_to_vec_embed':\n",
    "        X,y=gen_data_embed(pd_train,word2vec_model)\n",
    "    elif f_type == 'google_preprocess':\n",
    "        X,y=gen_data_google(pd_train)\n",
    "    elif f_type == 'tfidf_not_preprocess':\n",
    "        X,y=gen_data_new_tfidf2(pd_train)\n",
    "    elif f_type == 'tfidf_preprocess':\n",
    "        X,y=gen_data_new_tfidf(pd_train)\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_google_rem(pd_train)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_google_norem(pd_train)\n",
    "    elif f_type == 'google_preprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_norem_google_rem(pd_train)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_rem_google_norimportem(pd_train)\n",
    "    elif f_type == 'google_preprocess_embed':\n",
    "        X,y=combine_google_rem_embed(pd_train,word2vec_model)\n",
    "    elif f_type == 'tfidf_preprocess_embed':\n",
    "        X,y=combine_tf_rem_embed(pd_train,word2vec_model)\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess_embed':\n",
    "        ###best features####\n",
    "        X,y=combine_tf_rem_google_rem_embed(pd_train,word2vec_model)\n",
    "    else:\n",
    "        print(\"give correct feature selection\")    \n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models available\n",
    "1. decision_tree_classifier\n",
    "2. MLPClassifier\n",
    "3. KNeighborsClassifier\n",
    "4. ExtraTreeClassifier\n",
    "5. ExtraTreeClassifier_2\n",
    "6. RandomForestClassifier\n",
    "7. SVC\n",
    "8. CatboostClassifier\n",
    "9. XGB_classifier\n",
    "10. Logistic_Regression\n",
    "11. MLKNN\n",
    "12. MLARAM\n",
    "13. Gaussian Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Feature Models available\n",
    "1. google_not_preprocess\n",
    "2. word_to_vec_embed\n",
    "3. google_preprocess\n",
    "4. tfidf_not_preprocess\n",
    "5. tfidf_preprocess\n",
    "6. google_preprocess_tfidf_preprocess\n",
    "7. google_nopreprocess_tfidf_nopreprocess\n",
    "8. google_preprocess_tfidf_nopreprocess\n",
    "9. google_nopreprocess_tfidf_preprocess\n",
    "10. google_preprocess_embed\n",
    "11. tfidf_preprocess_embed\n",
    "12. google_preprocess_tfidf_preprocess_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the model and the feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select feature combination\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692304430a744b36910ebef0891e03f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(options=('google_not_preprocess', 'word_to_vec_embed', 'google_preprocess', 'tfidf_not_preprocess', 'tfidf_preprocess', 'google_preprocess_tfidf_preprocess', 'google_nopreprocess_tfidf_nopreprocess', 'google_preprocess_tfidf_nopreprocess', 'google_nopreprocess_tfidf_preprocess', 'google_preprocess_embed', 'tfidf_preprocess_embed', 'google_preprocess_tfidf_preprocess_embed'), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options_ft=['google_not_preprocess','word_to_vec_embed','google_preprocess','tfidf_not_preprocess','tfidf_preprocess','google_preprocess_tfidf_preprocess','google_nopreprocess_tfidf_nopreprocess','google_preprocess_tfidf_nopreprocess', 'google_nopreprocess_tfidf_preprocess','google_preprocess_embed','tfidf_preprocess_embed','google_preprocess_tfidf_preprocess_embed']\n",
    "ft= widgets.Dropdown(options=options_ft, value=None)\n",
    "print('select feature combination') \n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select a model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4f06af07944843bca596a6ae159130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(options=('decision_tree_classifier', 'MLPClassifier', 'KNeighborsClassifier', 'ExtraTreeClassifier', 'ExtraTreeClassifier_2', 'RandomForestClassifier', 'SVC', 'Catboost', 'XGB_classifier', 'Logistic_Regression', 'Gaussian Naive bayes', 'MLKNN', 'MLARAM'), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options_clf=['decision_tree_classifier','MLPClassifier','KNeighborsClassifier','ExtraTreeClassifier','ExtraTreeClassifier_2','RandomForestClassifier','SVC','Catboost','XGB_classifier','Logistic_Regression','Gaussian Naive bayes','MLKNN','MLARAM']\n",
    "clf= widgets.Dropdown(options=options_clf, value=None)\n",
    "print('select a model') \n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fifti'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###specify the model name\n",
    "abv_name=clf.value\n",
    "###specify the feature model###\n",
    "feature_select=ft.value\n",
    "#### model name ####\n",
    "model_name=clf.value\n",
    "\n",
    "### calling of the method####\n",
    "classification_model(abv_name,feature_select,model_name,save_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:punyajoy-nogpu]",
   "language": "python",
   "name": "conda-env-punyajoy-nogpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
